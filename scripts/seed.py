#!/usr/bin/env python3
"""
Data seeding script for YouTube Search Clone (World Search System).
Scrapes data from Wikipedia, ArXiv, Google Books, and other APIs, then inserts into PostgreSQL.
Adapted from pg_trgm_demo project.
"""

import requests
from bs4 import BeautifulSoup
import psycopg2
import time
import re
import xml.etree.ElementTree as ET
import argparse
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import urllib3
from datetime import datetime

# Disable SSL warnings for APIs with certificate issues
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def log(message, prefix="â„¹ï¸"):
    """Print log message with timestamp"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {prefix} {message}", flush=True)

# Database connection parameters for YouTube Search Clone
# Matches docker-compose.yml configuration
DB_PARAMS = {
    'host': 'localhost',
    'port': 5433,  # External port from docker-compose.yml
    'database': 'worlds_db',
    'user': 'worlds_user',
    'password': 'worlds_password'
}

def scrape_wikipedia_books():
    """Scrape best-selling books from Wikipedia using batch API (optimized)"""
    start_time = time.time()
    print("\nScraping Wikipedia best-selling books...")
    print("  â†’ Fetching list page...", end=' ', flush=True)
    url = "https://en.wikipedia.org/wiki/List_of_best-selling_books"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        print("âœ“")
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # First pass: collect all titles
        print("  â†’ Collecting book titles...", end=' ', flush=True)
        titles = []
        tables = soup.find_all('table', {'class': 'wikitable'})
        
        for table in tables[:3]:  # Process first 3 tables
            rows = table.find_all('tr')[1:]  # Skip header row
            
            for row in rows[:30]:  # Limit per table
                cells = row.find_all(['td', 'th'])
                if len(cells) < 1:
                    continue
                
                # Get book title from first column
                first_cell = cells[0]
                link = first_cell.find('a')
                
                if link and link.get('href'):
                    title = link.get_text(strip=True)
                    # Clean up title
                    title = re.sub(r'\[.*?\]', '', title).strip()
                    
                    if title and len(title) > 2:
                        titles.append(title)
                
                if len(titles) >= 50:
                    break
            
            if len(titles) >= 50:
                break
        
        print(f"Found {len(titles)} titles")
        
        # Second pass: batch fetch descriptions using Wikipedia API
        # This is MUCH faster - only 1 request instead of 50!
        print(f"  â†’ Batch fetching descriptions (1 request for all {len(titles)} books)...", end=' ', flush=True)
        books = []
        
        # Wikipedia API can handle up to 50 titles per request
        batch_size = 50
        for i in range(0, len(titles), batch_size):
            batch_titles = titles[i:i+batch_size]
            
            api_url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'titles': '|'.join(batch_titles),  # Join titles with |
                'prop': 'extracts',
                'exintro': True,  # Only intro section
                'explaintext': True,  # Plain text
                'exsentences': 3  # First 3 sentences
            }
            
            api_response = requests.get(api_url, params=params, headers=headers, timeout=15)
            if api_response.status_code == 200:
                data = api_response.json()
                pages = data.get('query', {}).get('pages', {})
                
                for page_id, page_data in pages.items():
                    if page_id == '-1':  # Page not found
                        continue
                    title = page_data.get('title', '')
                    extract = page_data.get('extract', '')
                    if title and extract and len(extract) > 50:
                        # Limit description length
                        books.append((title, extract[:500]))
        
        print(f"âœ“ Got {len(books)} descriptions")
        elapsed_time = time.time() - start_time
        print(f"âœ“ Total: {len(books)} books from Wikipedia (optimized: 2 requests vs 51 before)")
        print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
        return books
    
    except Exception as e:
        print(f"\nâœ— Error scraping Wikipedia: {e}")
        return []

def scrape_arxiv_papers(target_count=4000, max_workers=5):
    """
    Scrape academic papers from ArXiv API with parallel processing.
    Returns papers with title and abstract (description).
    """
    start_time = time.time()
    print(f"\nScraping ArXiv papers (Target: {target_count}, Parallel workers: {max_workers})...")
    papers = []
    seen_titles = set()
    lock = threading.Lock()  # Thread-safe operations
    
    # ArXiv categories - diverse fields
    categories = [
        'cs.AI', 'cs.LG', 'cs.CL', 'cs.CV', 'cs.NE', 'cs.RO',  # Computer Science
        'physics:cond-mat', 'physics:astro-ph', 'physics:hep-th',  # Physics
        'math.CO', 'math.AG', 'math.NT',  # Mathematics
        'q-bio.GN', 'q-bio.NC',  # Quantitative Biology
        'stat.ML', 'econ.EM'  # Statistics & Economics
    ]
    
    def fetch_arxiv_batch(category, start, batch_num):
        """Fetch a single batch from ArXiv"""
        try:
            url = f'http://export.arxiv.org/api/query?search_query=cat:{category}&start={start}&max_results=100'
            response = requests.get(url, timeout=15)
            
            if response.status_code != 200:
                return []
            
            # Parse XML response
            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = root.findall('atom:entry', ns)
            
            batch_papers = []
            for entry in entries:
                title_elem = entry.find('atom:title', ns)
                summary_elem = entry.find('atom:summary', ns)
                
                if title_elem is not None and summary_elem is not None:
                    title = title_elem.text.replace('\n', ' ').strip()
                    summary = summary_elem.text.replace('\n', ' ').strip()
                    
                    if len(summary) > 100:
                        batch_papers.append((title, summary))
            
            return batch_papers
            
        except Exception as e:
            print(f"\n    âœ— Error in batch {batch_num}: {e}")
            return []
    
    # Process categories
    for idx, category in enumerate(categories, 1):
        with lock:
            if len(papers) >= target_count:
                print(f"  âœ“ Target reached! Skipping remaining categories...")
                break
        
        print(f"  [{idx}/{len(categories)}] Category: {category} - Parallel fetching...", end=' ', flush=True)
        category_start = len(papers)
        
        # Create tasks for parallel execution
        tasks = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for batch_num, start in enumerate(range(0, 500, 100), 1):
                if len(papers) >= target_count:
                    break
                future = executor.submit(fetch_arxiv_batch, category, start, batch_num)
                tasks.append(future)
            
            # Collect results as they complete
            for future in as_completed(tasks):
                batch_papers = future.result()
                
                with lock:
                    for title, summary in batch_papers:
                        title_lower = title.lower()
                        if title_lower not in seen_titles:
                            seen_titles.add(title_lower)
                            papers.append((title, summary))
                            
                            if len(papers) >= target_count:
                                break
        
        category_added = len(papers) - category_start
        print(f"âœ“ Added {category_added}, Total: {len(papers)}/{target_count}")
        
        # Brief pause between categories
        time.sleep(0.5)
    
    elapsed_time = time.time() - start_time
    print(f"âœ“ Total ArXiv papers collected: {len(papers)}")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return papers

def scrape_wikipedia_bulk(target_count=4000, max_workers=30):
    """
    Scrape random Wikipedia articles using optimized batch API.
    Uses list=random (500 IDs) + batch content fetch (50 per request).
    Much faster: ~400-450 articles per 11 requests vs ~15-18 per request.
    """
    start_time = time.time()
    print(f"\nScraping Wikipedia articles (Target: {target_count}, Parallel workers: {max_workers})...")
    articles = []
    seen_titles = set()
    lock = threading.Lock()
    
    def fetch_wikipedia_batch_optimized():
        """
        Optimized: Fetch 500 random page IDs, then batch query content.
        Returns ~400-450 articles per call (vs ~15-18 before).
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            # Step 1: Get 500 random page IDs
            url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'random',
                'rnnamespace': 0,
                'rnlimit': 500  # Max 500 random pages
            }
            
            response = requests.get(url, params=params, headers=headers, timeout=15)
            if response.status_code != 200:
                return []
            
            data = response.json()
            random_pages = data.get('query', {}).get('random', [])
            page_ids = [str(page['id']) for page in random_pages]
            
            if not page_ids:
                return []
            
            # Step 2: Batch fetch content (50 pages per request, max limit)
            batch_articles = []
            for i in range(0, len(page_ids), 50):
                batch_ids = page_ids[i:i+50]
                
                content_params = {
                    'action': 'query',
                    'format': 'json',
                    'pageids': '|'.join(batch_ids),
                    'prop': 'extracts',
                    'exintro': True,
                    'explaintext': True,
                    'exsentences': 5
                }
                
                content_response = requests.get(url, params=content_params, headers=headers, timeout=15)
                if content_response.status_code != 200:
                    continue
                
                content_data = content_response.json()
                pages = content_data.get('query', {}).get('pages', {})
                
                for page_id, page_data in pages.items():
                    title = page_data.get('title', '')
                    extract = page_data.get('extract', '')
                    
                    # Relaxed filter: 50 chars (was 100)
                    if (title and extract and 
                        len(extract) > 50 and
                        'may refer to' not in extract):
                        batch_articles.append((title, extract))
            
            return batch_articles
            
        except Exception as e:
            return []
    
    # Calculate batches needed (each batch now gets ~180-200 articles on average)
    # Conservative estimate to ensure we reach target
    batches_needed = (target_count // 180) + 3
    
    print(f"  â†’ Launching {batches_needed} parallel super-batches (each fetches ~180-200 articles)...", flush=True)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit tasks
        futures = [executor.submit(fetch_wikipedia_batch_optimized) for _ in range(batches_needed)]
        
        # Collect results as they complete
        completed = 0
        for future in as_completed(futures):
            batch_articles = future.result()
            completed += 1
            
            with lock:
                for title, extract in batch_articles:
                    title_lower = title.lower()
                    if title_lower not in seen_titles:
                        seen_titles.add(title_lower)
                        articles.append((title, extract))
                        
                        if len(articles) >= target_count:
                            break
                
                # Show progress
                progress_pct = (len(articles) / target_count) * 100
                print(f"  ğŸ“Š Super-batch {completed}/{batches_needed} complete, Articles: {len(articles)}/{target_count} ({progress_pct:.1f}%)")
                
                if len(articles) >= target_count:
                    break
    
    elapsed_time = time.time() - start_time
    print(f"âœ“ Total Wikipedia articles collected: {len(articles)} (Optimized: ~25x faster)")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return articles[:target_count]  # Ensure we don't exceed target

def scrape_google_books_free(target_count=2000, max_workers=5):
    """
    Use Google Books public API to scrape book descriptions with parallel processing.
    Free and no API key required.
    """
    start_time = time.time()
    print(f"\nScraping Google Books (Target: {target_count}, Parallel workers: {max_workers})...")
    books = []
    seen_titles = set()
    lock = threading.Lock()
    
    # Expanded list of subjects for more diversity
    subjects = [
        'fiction', 'history', 'science', 'programming', 'art', 'cooking', 'travel', 
        'fantasy', 'mystery', 'philosophy', 'psychology', 'business', 'economics',
        'medicine', 'biology', 'chemistry', 'physics', 'mathematics', 'engineering',
        'literature', 'poetry', 'drama', 'music', 'architecture', 'photography',
        'religion', 'sociology', 'anthropology', 'education', 'law'
    ]
    
    def fetch_google_books_page(subject, start_index):
        """Fetch a single page of Google Books results"""
        try:
            url = f"https://www.googleapis.com/books/v1/volumes?q=subject:{subject}&startIndex={start_index}&maxResults=40&langRestrict=en"
            response = requests.get(url, timeout=10)
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            items = data.get('items', [])
            
            page_books = []
            for item in items:
                info = item.get('volumeInfo', {})
                title = info.get('title')
                description = info.get('description')
                
                if title and description and len(description) > 50:
                    page_books.append((title, description))
            
            return page_books
            
        except Exception as e:
            return []
    
    for idx, subject in enumerate(subjects, 1):
        with lock:
            if len(books) >= target_count:
                print(f"  âœ“ Target reached! Skipping remaining subjects...")
                break
        
        print(f"  [{idx}/{len(subjects)}] Subject: {subject} - Parallel fetching...", end=' ', flush=True)
        subject_start = len(books)
        
        # Create tasks for parallel execution (fetch multiple pages at once)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for start_index in range(0, 200, 40):  # 5 pages per subject
                if len(books) >= target_count:
                    break
                future = executor.submit(fetch_google_books_page, subject, start_index)
                futures.append(future)
            
            # Collect results
            for future in as_completed(futures):
                page_books = future.result()
                
                with lock:
                    for title, description in page_books:
                        title_lower = title.lower()
                        if title_lower not in seen_titles:
                            seen_titles.add(title_lower)
                            books.append((title, description))
                            
                            if len(books) >= target_count:
                                break
        
        subject_added = len(books) - subject_start
        print(f"âœ“ +{subject_added} (Total: {len(books)}/{target_count})")
        
        # Brief pause between subjects to avoid rate limiting
        time.sleep(0.3)
    
    elapsed_time = time.time() - start_time
    print(f"âœ“ Total Google Books collected: {len(books)}")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return books

def scrape_quotable_quotes(target_count=1500):
    """
    Scrape inspirational quotes from Quotable.io API.
    Free API, no key required (SSL certificate bypass needed).
    """
    start_time = time.time()
    log(f"é–‹å§‹æŠ“å– Quotable.io åè¨€ (ç›®æ¨™: {target_count} ç­†)")
    print(f"\nScraping quotes from Quotable.io (Target: {target_count})...")
    quotes = []
    seen_quotes = set()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    attempts = 0
    max_attempts = target_count * 2  # Allow retries
    last_log_time = time.time()
    
    log(f"é–‹å§‹é€ç­†è«‹æ±‚åè¨€ (æ¯æ¬¡è«‹æ±‚é–“éš” 0.2 ç§’ï¼Œé è¨ˆéœ€è¦ {target_count * 0.2 / 60:.1f} åˆ†é˜)")
    
    while len(quotes) < target_count and attempts < max_attempts:
        attempts += 1
        
        try:
            url = "https://api.quotable.io/random"
            # Use verify=False to bypass SSL certificate verification
            response = requests.get(url, headers=headers, timeout=10, verify=False)
            
            if response.status_code == 200:
                data = response.json()
                author = data.get('author', 'Unknown')
                content = data.get('content', '')
                
                # Check for duplicates and minimum length
                if content and content not in seen_quotes and len(content) > 20:
                    seen_quotes.add(content)
                    title = f"Quote by {author}"
                    description = f'"{content}" - {author}'
                    quotes.append((title, description))
                    
                    if len(quotes) % 100 == 0:
                        progress_pct = (len(quotes) / target_count) * 100
                        log(f"Quotable é€²åº¦: {len(quotes)}/{target_count} ({progress_pct:.1f}%)", "ğŸ“")
            
            # Log every 30 seconds to show it's still working
            current_time = time.time()
            if current_time - last_log_time > 30:
                log(f"æŒçºŒæŠ“å–ä¸­... å·²å®Œæˆ {len(quotes)}/{target_count} (å˜—è©¦ {attempts} æ¬¡)", "â³")
                last_log_time = current_time
            
            # Rate limiting
            time.sleep(0.2)
            
        except Exception as e:
            if attempts % 500 == 0:
                log(f"è«‹æ±‚å¤±æ•— (å·²å˜—è©¦ {attempts} æ¬¡): {str(e)[:50]}", "âš ï¸")
            continue
    
    elapsed_time = time.time() - start_time
    log(f"âœ“ Quotable å®Œæˆ: æ”¶é›† {len(quotes)} ç­†åè¨€ï¼Œè€—æ™‚ {elapsed_time:.1f} ç§’", "âœ…")
    print(f"âœ“ Total quotes collected: {len(quotes)}")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return quotes

def scrape_random_facts(target_count=1000):
    """
    Scrape random interesting facts from UselessFacts API.
    Free API, no key required.
    """
    start_time = time.time()
    log(f"é–‹å§‹æŠ“å– UselessFacts å†·çŸ¥è­˜ (ç›®æ¨™: {target_count} ç­†)")
    print(f"\nScraping random facts from UselessFacts (Target: {target_count})...")
    facts = []
    seen_facts = set()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    attempts = 0
    max_attempts = target_count * 2
    last_log_time = time.time()
    
    log(f"é–‹å§‹é€ç­†è«‹æ±‚å†·çŸ¥è­˜ (æ¯æ¬¡è«‹æ±‚é–“éš” 0.2 ç§’ï¼Œé è¨ˆéœ€è¦ {target_count * 0.2 / 60:.1f} åˆ†é˜)")
    
    while len(facts) < target_count and attempts < max_attempts:
        attempts += 1
        
        try:
            url = "https://uselessfacts.jsph.pl/random.json?language=en"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                fact = data.get('text', '')
                
                # Check for duplicates and minimum length
                if fact and fact not in seen_facts and len(fact) > 20:
                    seen_facts.add(fact)
                    
                    # Generate title from first few words
                    title_words = fact.split()[:8]
                    title = ' '.join(title_words)
                    if len(fact.split()) > 8:
                        title += '...'
                    
                    facts.append((title, fact))
                    
                    if len(facts) % 100 == 0:
                        progress_pct = (len(facts) / target_count) * 100
                        log(f"UselessFacts é€²åº¦: {len(facts)}/{target_count} ({progress_pct:.1f}%)", "ğŸ²")
            
            # Log every 30 seconds
            current_time = time.time()
            if current_time - last_log_time > 30:
                log(f"æŒçºŒæŠ“å–ä¸­... å·²å®Œæˆ {len(facts)}/{target_count} (å˜—è©¦ {attempts} æ¬¡)", "â³")
                last_log_time = current_time
            
            # Rate limiting
            time.sleep(0.2)
            
        except Exception as e:
            if attempts % 500 == 0:
                log(f"è«‹æ±‚å¤±æ•— (å·²å˜—è©¦ {attempts} æ¬¡)", "âš ï¸")
            continue
    
    elapsed_time = time.time() - start_time
    log(f"âœ“ UselessFacts å®Œæˆ: æ”¶é›† {len(facts)} ç­†å†·çŸ¥è­˜ï¼Œè€—æ™‚ {elapsed_time:.1f} ç§’", "âœ…")
    print(f"âœ“ Total facts collected: {len(facts)}")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return facts

def scrape_zenquotes(target_count=500):
    """
    Scrape quotes from ZenQuotes API (alternative quote source).
    Free API, no key required, but has strict rate limit (5 requests per 30 seconds).
    """
    start_time = time.time()
    log(f"é–‹å§‹æŠ“å– ZenQuotes åè¨€ (ç›®æ¨™: {target_count} ç­†)")
    print(f"\nScraping quotes from ZenQuotes (Target: {target_count})...")
    quotes = []
    seen_quotes = set()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    attempts = 0
    max_attempts = target_count * 2
    request_count = 0
    last_log_time = time.time()
    
    # Calculate estimated time (5 requests per 6 seconds = ~120 per minute)
    estimated_minutes = target_count / 120
    log(f"âš ï¸  ZenQuotes æœ‰åš´æ ¼é€Ÿç‡é™åˆ¶ (æ¯ 30 ç§’ 5 æ¬¡è«‹æ±‚)ï¼Œé è¨ˆéœ€è¦ {estimated_minutes:.1f} åˆ†é˜")
    
    while len(quotes) < target_count and attempts < max_attempts:
        attempts += 1
        
        try:
            url = "https://zenquotes.io/api/random"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data and len(data) > 0:
                    quote = data[0]
                    author = quote.get('a', 'Unknown')
                    content = quote.get('q', '')
                    
                    # Check for duplicates and minimum length
                    if content and content not in seen_quotes and len(content) > 20:
                        seen_quotes.add(content)
                        title = f"Quote by {author}"
                        description = f'"{content}" - {author}'
                        quotes.append((title, description))
                        
                        if len(quotes) % 50 == 0:
                            progress_pct = (len(quotes) / target_count) * 100
                            log(f"ZenQuotes é€²åº¦: {len(quotes)}/{target_count} ({progress_pct:.1f}%)", "ğŸ’­")
            
            # ZenQuotes rate limit: 5 requests per 30 seconds
            request_count += 1
            if request_count % 5 == 0:
                log(f"é”åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… 6 ç§’... (å·²å®Œæˆ {len(quotes)}/{target_count})", "â¸ï¸")
                time.sleep(6)  # Wait 6 seconds every 5 requests
            else:
                time.sleep(0.5)
            
            # Log every 30 seconds
            current_time = time.time()
            if current_time - last_log_time > 30:
                log(f"æŒçºŒæŠ“å–ä¸­... å·²å®Œæˆ {len(quotes)}/{target_count}", "â³")
                last_log_time = current_time
            
        except Exception as e:
            if attempts % 100 == 0:
                log(f"è«‹æ±‚å¤±æ•— (å·²å˜—è©¦ {attempts} æ¬¡)", "âš ï¸")
            continue
    
    elapsed_time = time.time() - start_time
    log(f"âœ“ ZenQuotes å®Œæˆ: æ”¶é›† {len(quotes)} ç­†åè¨€ï¼Œè€—æ™‚ {elapsed_time:.1f} ç§’", "âœ…")
    print(f"âœ“ Total ZenQuotes collected: {len(quotes)}")
    print(f"â±ï¸  Time taken: {elapsed_time:.2f} seconds")
    return quotes

def insert_worlds_to_db(worlds):
    """
    Insert worlds into PostgreSQL database.
    Adapted for YouTube Search Clone schema with UUID primary keys.
    Note: Database and schema are already created by docker-compose migrations.
    """
    print("\n" + "="*60)
    print("DATABASE OPERATIONS")
    print("="*60)
    
    try:
        log("é–‹å§‹è³‡æ–™åº«æ“ä½œ", "ğŸ’¾")
        print("â†’ Connecting to PostgreSQL...", end=' ', flush=True)
        log(f"é€£ç·šåˆ° PostgreSQL ({DB_PARAMS['host']}:{DB_PARAMS['port']}/{DB_PARAMS['database']})", "ğŸ”Œ")
        conn = psycopg2.connect(**DB_PARAMS)
        cur = conn.cursor()
        print("âœ“")
        log("âœ“ è³‡æ–™åº«é€£ç·šæˆåŠŸ", "âœ…")
        
        print("â†’ Clearing existing data...", end=' ', flush=True)
        log("æ¸…é™¤ worlds è¡¨ä¸­çš„ç¾æœ‰è³‡æ–™...", "ğŸ—‘ï¸")
        cur.execute("DELETE FROM worlds")
        conn.commit()
        print("âœ“")
        log("âœ“ èˆŠè³‡æ–™å·²æ¸…é™¤", "âœ…")
        
        print(f"â†’ Inserting {len(worlds)} records...")
        log(f"é–‹å§‹æ’å…¥ {len(worlds)} ç­†è³‡æ–™ (æ‰¹æ¬¡å¤§å°: 100)", "ğŸ“¥")
        batch_size = 100
        insert_start = time.time()
        last_log_time = time.time()
        
        for i in range(0, len(worlds), batch_size):
            batch = worlds[i:i+batch_size]
            for title, description in batch:
                # Insert with auto-generated UUID and timestamp
                cur.execute(
                    "INSERT INTO worlds (title, description) VALUES (%s, %s)",
                    (title, description)
                )
            conn.commit()
            progress = min(i + batch_size, len(worlds))
            progress_pct = (progress / len(worlds)) * 100
            print(f"  Progress: {progress}/{len(worlds)} ({progress_pct:.1f}%)")
            
            # Log every 2000 records or every 10 seconds
            current_time = time.time()
            if progress % 2000 == 0 or (current_time - last_log_time > 10):
                elapsed = current_time - insert_start
                rate = progress / elapsed if elapsed > 0 else 0
                remaining = (len(worlds) - progress) / rate if rate > 0 else 0
                log(f"æ’å…¥é€²åº¦: {progress}/{len(worlds)} ({progress_pct:.1f}%) - é€Ÿç‡: {rate:.0f} ç­†/ç§’ - é è¨ˆå‰©é¤˜: {remaining:.0f} ç§’", "â±ï¸")
                last_log_time = current_time
        
        insert_elapsed = time.time() - insert_start
        log(f"âœ“ è³‡æ–™æ’å…¥å®Œæˆï¼Œè€—æ™‚ {insert_elapsed:.1f} ç§’", "âœ…")
        
        # Get count
        log("é©—è­‰è³‡æ–™ç­†æ•¸...", "ğŸ”")
        cur.execute("SELECT COUNT(*) FROM worlds")
        count = cur.fetchone()[0]
        print(f"âœ“ Successfully inserted {count} records")
        log(f"âœ“ ç¢ºèª: è³‡æ–™åº«ä¸­å…±æœ‰ {count} ç­†è³‡æ–™", "âœ…")
        
        # Note: Indexes are already created by migrations (001_init.sql and 002_add_bigm.sql)
        print("\nâœ“ Indexes already exist from migrations")
        log("âœ“ ç´¢å¼•å·²ç”± migrations å»ºç«‹ï¼ˆpg_trgm å’Œ pg_bigmï¼‰", "ğŸ“‡")
        
        cur.close()
        conn.close()
        log("è³‡æ–™åº«é€£ç·šå·²é—œé–‰", "ğŸ”Œ")
        
        print("\n" + "="*60)
        print("ğŸ‰ DATABASE SEEDING COMPLETED!")
        print("="*60)
        log("ğŸ‰ è³‡æ–™åº«ç¨®å­è³‡æ–™å¡«å……å®Œæˆï¼", "ğŸ‰")
        
    except Exception as e:
        log(f"âœ— è³‡æ–™åº«éŒ¯èª¤: {str(e)[:200]}", "âŒ")
        print(f"\nâœ— Database error: {e}")
        print(f"   Make sure PostgreSQL is running (docker-compose up -d postgres)")
        if 'conn' in locals():
            conn.rollback()
            conn.close()

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='YouTube Search Clone - Data Seeding Script',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # æŠ“å– 10,000 ç­†è³‡æ–™ (é è¨­ï¼Œä¸¦è¡Œæ¨¡å¼)
  python seed.py
  
  # ä½¿ç”¨éä¸¦è¡Œæ¨¡å¼ï¼ˆä¾åºæŠ“å–ï¼Œè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰
  python seed.py --no-parallel
  
  # æŠ“å– 1,000 ç­†è³‡æ–™ï¼ˆä¸¦è¡Œæ¨¡å¼ï¼‰
  python seed.py --total 1000
  
  # æŠ“å– 1,000 ç­†è³‡æ–™ï¼ˆéä¸¦è¡Œæ¨¡å¼ï¼‰
  python seed.py --total 1000 --no-parallel
  
  # è‡ªè¨‚å„ä¾†æºæ•¸é‡ï¼ˆä¸¦è¡Œæ¨¡å¼ï¼‰
  python seed.py --arxiv 2500 --wikipedia 2500 --books 2000 \\
                 --quotable 1500 --facts 1000 --zenquotes 500
  
  # è‡ªè¨‚å„ä¾†æºæ•¸é‡ï¼ˆéä¸¦è¡Œæ¨¡å¼ï¼‰
  python seed.py --arxiv 2500 --wikipedia 2500 --books 2000 \\
                 --quotable 1500 --facts 1000 --zenquotes 500 --no-parallel
  
  # åªæŠ“å–åè¨€å’Œå†·çŸ¥è­˜ï¼ˆéä¸¦è¡Œæ¨¡å¼ï¼Œé©åˆé™¤éŒ¯ï¼‰
  python seed.py --quotable 500 --facts 500 --zenquotes 100 \\
                 --arxiv 0 --wikipedia 0 --books 0 --no-parallel
  
  # å¿«é€Ÿæ¸¬è©¦ (100 ç­†ï¼Œè‡ªå‹•åˆ†é…)
  python seed.py --total 100
  
  # è·³é Wikipedia æš¢éŠ·æ›¸
  python seed.py --skip-wiki-bestsellers
        '''
    )
    
    parser.add_argument(
        '--total', 
        type=int, 
        default=10000,
        help='ç¸½è³‡æ–™ç­†æ•¸ç›®æ¨™ (é è¨­: 10000)ã€‚æœƒè‡ªå‹•åˆ†é…çµ¦å„ä¾†æº'
    )
    
    parser.add_argument(
        '--arxiv',
        type=int,
        help='ArXiv è«–æ–‡æ•¸é‡ (å­¸è¡“æ‘˜è¦)'
    )
    
    parser.add_argument(
        '--wikipedia',
        type=int,
        help='Wikipedia æ¢ç›®æ•¸é‡ (ç™¾ç§‘å…¨æ›¸)'
    )
    
    parser.add_argument(
        '--books',
        type=int,
        help='Google Books æ•¸é‡ (æ›¸ç±ç°¡ä»‹)'
    )
    
    parser.add_argument(
        '--quotable',
        type=int,
        help='Quotable.io åè¨€æ•¸é‡ (å‹µå¿—åè¨€)'
    )
    
    parser.add_argument(
        '--facts',
        type=int,
        help='UselessFacts å†·çŸ¥è­˜æ•¸é‡ (æœ‰è¶£äº‹å¯¦)'
    )
    
    parser.add_argument(
        '--zenquotes',
        type=int,
        help='ZenQuotes åè¨€æ•¸é‡ (é¡å¤–åè¨€ä¾†æº)'
    )
    
    parser.add_argument(
        '--skip-wiki-bestsellers',
        action='store_true',
        help='è·³é Wikipedia æš¢éŠ·æ›¸æ¸…å–® (ç´„ 50 ç­†)'
    )
    
    parser.add_argument(
        '--no-parallel',
        action='store_true',
        help='åœç”¨ä¸¦è¡Œæ¨¡å¼ï¼Œä¾åºæŠ“å–å„ä¾†æºï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœä½¿ç”¨è€…æŒ‡å®šäº†å€‹åˆ¥ä¾†æºæ•¸é‡ï¼Œå‰‡ä½¿ç”¨æŒ‡å®šå€¼
    if (args.arxiv is not None or args.wikipedia is not None or args.books is not None or
        args.quotable is not None or args.facts is not None or args.zenquotes is not None):
        arxiv_count = args.arxiv if args.arxiv is not None else 0
        wiki_count = args.wikipedia if args.wikipedia is not None else 0
        books_count = args.books if args.books is not None else 0
        quotable_count = args.quotable if args.quotable is not None else 0
        facts_count = args.facts if args.facts is not None else 0
        zenquotes_count = args.zenquotes if args.zenquotes is not None else 0
    else:
        # å¦å‰‡æ ¹æ“š total è‡ªå‹•åˆ†é…
        # 25% ArXiv, 25% Wikipedia, 20% Books, 15% Quotable, 10% Facts, 5% ZenQuotes
        arxiv_count = int(args.total * 0.25)
        wiki_count = int(args.total * 0.25)
        books_count = int(args.total * 0.20)
        quotable_count = int(args.total * 0.15)
        facts_count = int(args.total * 0.10)
        zenquotes_count = int(args.total * 0.05)
    
    return {
        'arxiv': arxiv_count,
        'wikipedia': wiki_count,
        'books': books_count,
        'quotable': quotable_count,
        'facts': facts_count,
        'zenquotes': zenquotes_count,
        'skip_bestsellers': args.skip_wiki_bestsellers,
        'parallel': not args.no_parallel,
        'total_target': args.total
    }

def main():
    # Parse command line arguments
    log("ğŸš€ å•Ÿå‹• YouTube Search Clone è³‡æ–™ç¨®å­è…³æœ¬", "ğŸš€")
    config = parse_arguments()
    
    # Record total start time
    total_start_time = time.time()
    
    print("=" * 60)
    print("YouTube Search Clone - Data Seeding")
    print("=" * 60)
    log(f"ç›®æ¨™è¨­å®š: å…± ~{config['total_target']} ç­†è³‡æ–™", "ğŸ¯")
    print(f"Target Configuration:")
    print(f"  ArXiv Papers: {config['arxiv']}")
    print(f"  Wikipedia Articles: {config['wikipedia']}")
    print(f"  Google Books: {config['books']}")
    print(f"  Quotable Quotes: {config['quotable']}")
    print(f"  Random Facts: {config['facts']}")
    print(f"  ZenQuotes: {config['zenquotes']}")
    print(f"  Wikipedia Bestsellers: {'No' if config['skip_bestsellers'] else 'Yes (~50)'}")
    print(f"  Execution Mode: {'PARALLEL' if config['parallel'] else 'SEQUENTIAL'}")
    print(f"  Total Target: ~{config['total_target']}")
    print("=" * 60)
    
    log(f"åŸ·è¡Œæ¨¡å¼: {'ä¸¦è¡Œ (PARALLEL)' if config['parallel'] else 'å¾ªåº (SEQUENTIAL)'}", "âš™ï¸")
    log("é–‹å§‹è³‡æ–™æ”¶é›†éšæ®µ...", "ğŸ“Š")
    
    if config['parallel']:
        print("\nğŸš€ PARALLEL MODE: All sources fetching simultaneously!\n")
        log("å•Ÿå‹•ä¸¦è¡Œæ¨¡å¼ï¼šæ‰€æœ‰ä¾†æºå°‡åŒæ™‚é–‹å§‹æŠ“å–", "ğŸš€")
    else:
        print("\nâ³ SEQUENTIAL MODE: Fetching sources one by one...\n")
        log("å•Ÿå‹•å¾ªåºæ¨¡å¼ï¼šå°‡ä¾åºæŠ“å–å„å€‹ä¾†æº", "â³")
    
    all_data = []
    arxiv_papers = []
    wiki_articles = []
    google_books = []
    quotable_quotes = []
    random_facts = []
    zen_quotes = []
    wiki_books = []
    
    if config['parallel']:
        # ========== ä¸¦è¡Œæ¨¡å¼ ==========
        # Use ThreadPoolExecutor to fetch from all sources in parallel
        # Increased max_workers to 7 to handle all sources simultaneously
        log("å»ºç«‹åŸ·è¡Œç·’æ±  (max_workers=7)ï¼Œæº–å‚™æäº¤ä»»å‹™...", "ğŸ”§")
        
        with ThreadPoolExecutor(max_workers=7) as executor:
            futures = {}
            
            # Submit tasks for each data source
            if config['arxiv'] > 0:
                futures['arxiv'] = executor.submit(scrape_arxiv_papers, config['arxiv'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: ArXiv ({config['arxiv']} ç­†)", "ğŸ“¤")
            
            if config['wikipedia'] > 0:
                futures['wikipedia'] = executor.submit(scrape_wikipedia_bulk, config['wikipedia'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: Wikipedia ({config['wikipedia']} ç­†)", "ğŸ“¤")
            
            if config['books'] > 0:
                futures['google_books'] = executor.submit(scrape_google_books_free, config['books'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: Google Books ({config['books']} ç­†)", "ğŸ“¤")
            
            if config['quotable'] > 0:
                futures['quotable'] = executor.submit(scrape_quotable_quotes, config['quotable'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: Quotable ({config['quotable']} ç­†)", "ğŸ“¤")
            
            if config['facts'] > 0:
                futures['facts'] = executor.submit(scrape_random_facts, config['facts'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: UselessFacts ({config['facts']} ç­†)", "ğŸ“¤")
            
            if config['zenquotes'] > 0:
                futures['zenquotes'] = executor.submit(scrape_zenquotes, config['zenquotes'])
                log(f"âœ“ å·²æäº¤ä»»å‹™: ZenQuotes ({config['zenquotes']} ç­†)", "ğŸ“¤")
            
            if not config['skip_bestsellers']:
                futures['wiki_books'] = executor.submit(scrape_wikipedia_books)
                log("âœ“ å·²æäº¤ä»»å‹™: Wikipedia Bestsellers (~50 ç­†)", "ğŸ“¤")
            
            log(f"æ‰€æœ‰ä»»å‹™å·²æäº¤ (å…± {len(futures)} å€‹ä¾†æº)ï¼Œç­‰å¾…åŸ·è¡Œå®Œæˆ...", "â³")
            
            # Collect results as they complete
            completed = 0
            for source, future in futures.items():
                try:
                    log(f"ç­‰å¾… {source} å®Œæˆ... ({completed+1}/{len(futures)})", "â³")
                    result = future.result()
                    completed += 1
                    if source == 'arxiv':
                        arxiv_papers = result
                        log(f"âœ… ArXiv å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'wikipedia':
                        wiki_articles = result
                        log(f"âœ… Wikipedia å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'google_books':
                        google_books = result
                        log(f"âœ… Google Books å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'quotable':
                        quotable_quotes = result
                        log(f"âœ… Quotable å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'facts':
                        random_facts = result
                        log(f"âœ… UselessFacts å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'zenquotes':
                        zen_quotes = result
                        log(f"âœ… ZenQuotes å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                    elif source == 'wiki_books':
                        wiki_books = result
                        log(f"âœ… Wikipedia Bestsellers å®Œæˆ: æ”¶é›† {len(result)} ç­†", "âœ…")
                except Exception as e:
                    log(f"âœ— {source} ç™¼ç”ŸéŒ¯èª¤: {str(e)[:100]}", "âŒ")
                    print(f"\nâœ— Error fetching {source}: {e}")
    
    else:
        # ========== éä¸¦è¡Œæ¨¡å¼ï¼ˆä¾åºåŸ·è¡Œï¼‰==========
        try:
            if config['arxiv'] > 0:
                arxiv_papers = scrape_arxiv_papers(config['arxiv'])
            
            if config['wikipedia'] > 0:
                wiki_articles = scrape_wikipedia_bulk(config['wikipedia'])
            
            if config['books'] > 0:
                google_books = scrape_google_books_free(config['books'])
            
            if config['quotable'] > 0:
                quotable_quotes = scrape_quotable_quotes(config['quotable'])
            
            if config['facts'] > 0:
                random_facts = scrape_random_facts(config['facts'])
            
            if config['zenquotes'] > 0:
                zen_quotes = scrape_zenquotes(config['zenquotes'])
            
            if not config['skip_bestsellers']:
                wiki_books = scrape_wikipedia_books()
                
        except Exception as e:
            print(f"\nâœ— Error during sequential fetching: {e}")
    
    # Combine all results
    all_data.extend(arxiv_papers)
    all_data.extend(wiki_articles)
    all_data.extend(google_books)
    all_data.extend(quotable_quotes)
    all_data.extend(random_facts)
    all_data.extend(zen_quotes)
    all_data.extend(wiki_books)
    
    # Calculate total data collection time
    data_collection_time = time.time() - total_start_time
    
    log("âœ… è³‡æ–™æ”¶é›†éšæ®µå®Œæˆï¼", "âœ…")
    print(f"\n{'='*60}")
    print(f"Data Collection Summary:")
    print(f"  ArXiv Papers: {len(arxiv_papers)}")
    print(f"  Wikipedia Articles: {len(wiki_articles)}")
    print(f"  Google Books: {len(google_books)}")
    print(f"  Quotable Quotes: {len(quotable_quotes)}")
    print(f"  Random Facts: {len(random_facts)}")
    print(f"  ZenQuotes: {len(zen_quotes)}")
    print(f"  Wikipedia Books: {len(wiki_books)}")
    print(f"  Total collected: {len(all_data)}")
    print(f"  â±ï¸  Total data collection time: {data_collection_time:.2f} seconds ({data_collection_time/60:.2f} minutes)")
    print(f"{'='*60}\n")
    log(f"ğŸ“Š ç¸½è¨ˆæ”¶é›†: {len(all_data)} ç­†è³‡æ–™ï¼Œè€—æ™‚ {data_collection_time/60:.1f} åˆ†é˜", "ğŸ“Š")
    
    # Remove duplicates based on title (case-insensitive)
    log("é–‹å§‹è³‡æ–™å»é‡è™•ç†...", "ğŸ”„")
    seen = set()
    unique_worlds = []
    duplicates = 0
    for title, desc in all_data:
        title_lower = title.lower()
        if title_lower not in seen and desc:  # Ensure description exists
            seen.add(title_lower)
            unique_worlds.append((title, desc))
        else:
            duplicates += 1
    
    log(f"âœ“ å»é‡å®Œæˆ: åŸå§‹ {len(all_data)} ç­† â†’ å”¯ä¸€ {len(unique_worlds)} ç­† (ç§»é™¤ {duplicates} ç­†é‡è¤‡)", "âœ…")
    print(f"Total unique entries after deduplication: {len(unique_worlds)}")
    
    if len(unique_worlds) < 10:
        log("âš ï¸  è­¦å‘Š: æ”¶é›†çš„è³‡æ–™å°‘æ–¼ 10 ç­†ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š", "âš ï¸")
        print("Warning: Less than 10 entries scraped. Please check your internet connection.")
        return
    
    # Insert into database (no need to create database, it's handled by docker-compose)
    log(f"æº–å‚™å°‡ {len(unique_worlds)} ç­†è³‡æ–™å¯«å…¥è³‡æ–™åº«", "ğŸ’¾")
    insert_worlds_to_db(unique_worlds)

if __name__ == "__main__":
    main()

